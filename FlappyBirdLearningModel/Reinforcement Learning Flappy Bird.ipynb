{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION <tensorflow.python.client.session.Session object at 0x000001B950C25978>\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9179752214975792453\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import tensorflow as tf\n",
    " \n",
    "print(\"VERSION\", tf.Session(config=tf.ConfigProto(log_device_placement=True)))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path variables\n",
    "game_url = \"http://flappybird.io/\"\n",
    "game_url = \"http://users.csc.calpoly.edu/~nfgriffi/Play%20Flappy%20Bird.html\"\n",
    "chrome_driver_path = \"./chromedriver\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementsByClassName('canvas-container')[0].id = 'canvas-container'\"\n",
    "\n",
    "#get image from canvas\n",
    "# getbase64Script = \"canvasContainer = document.getElementById('canvas-container'); \\\n",
    "# return canvasContainer.toDataURL().substring(22)\"\n",
    "getbase64Script = \"return document.getElementById('testCanvas').toDataURL().substring(22)\"\n",
    "\n",
    "alive_t0 = 0\n",
    "alive_tf = 0\n",
    "max_score = 0\n",
    "cur_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.get(game_url)\n",
    "        # self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return dead\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return started\")\n",
    "    def restart(self):\n",
    "        global alive_t0\n",
    "        self._driver.execute_script(\"spacebar();\")\n",
    "        alive_t0 = int(round(time.time() * 1000))\n",
    "    def press_up(self):\n",
    "        self._driver.execute_script(\"spacebar();\")\n",
    "    def get_score(self):\n",
    "        global alive_t0\n",
    "        global alive_tf\n",
    "        alive_tf = int(round(time.time() * 1000))\n",
    "        score = alive_tf - alive_t0\n",
    "#         score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return score\n",
    "    def get_max_pipes(self):\n",
    "        pipes_str = self._driver.execute_script(\"highScore.text\")\n",
    "        pipes_int = int(pipes_str)\n",
    "        return pipes_int\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"spacebar();\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"spacebar();\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        global max_score, cur_score\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        is_over = False #game over\n",
    "        reward = 0.1               \n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "            reward = -0.05\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            is_over = True\n",
    "            if (score > max_score):\n",
    "                max_score = score\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -1\n",
    "            print(\"\\nCURRENT SCORE: {}\\n\\n\".format(cur_score))\n",
    "            cur_score = 0\n",
    "            self._agent.jump()\n",
    "        else:\n",
    "            cur_score = cur_score + reward\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver):\n",
    "    image_b64 = _driver.execute_script(getbase64Script)\n",
    "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:1200, :1000] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call only once to init file structure\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "#init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    time.sleep(3)\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)    \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 36000 / STATE explore / EPSILON 0.06413689899994521 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.04724569 / Loss  0.3726523518562317\n",
      "fps: 0.07808385169663595\n",
      "TIMESTEP 36001 / STATE explore / EPSILON 0.0641358999999452 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.15054742 / Loss  0.06273305416107178\n",
      "fps: 5.570124833997344\n",
      "TIMESTEP 36002 / STATE explore / EPSILON 0.0641349009999452 / ACTION 1 / REWARD -0.05 / Q_MAX  0.24945481 / Loss  0.028219174593687057\n",
      "fps: 5.898340456110893\n",
      "TIMESTEP 36003 / STATE explore / EPSILON 0.0641339019999452 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.13330732 / Loss  0.03791334480047226\n",
      "fps: 5.509881994080656\n",
      "TIMESTEP 36004 / STATE explore / EPSILON 0.0641329029999452 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.14835438 / Loss  0.020998820662498474\n",
      "fps: 5.570590899371395\n",
      "TIMESTEP 36005 / STATE explore / EPSILON 0.0641319039999452 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.14225365 / Loss  0.02273983135819435\n",
      "fps: 5.7297434643447\n",
      "TIMESTEP 36006 / STATE explore / EPSILON 0.0641309049999452 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.32777336 / Loss  0.0546058788895607\n",
      "fps: 5.601015961872052\n",
      "TIMESTEP 36007 / STATE explore / EPSILON 0.0641299059999452 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.08579837 / Loss  0.04764395207166672\n",
      "fps: 6.8665988897019945\n",
      "TIMESTEP 36008 / STATE explore / EPSILON 0.0641289069999452 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.37845588 / Loss  0.048191241919994354\n",
      "fps: 5.72961823072268\n",
      "TIMESTEP 36009 / STATE explore / EPSILON 0.0641279079999452 / ACTION 1 / REWARD -0.05 / Q_MAX  0.0065974854 / Loss  0.7297335863113403\n",
      "fps: 5.698000681971632\n",
      "TIMESTEP 36010 / STATE explore / EPSILON 0.06412690899994519 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.022298217 / Loss  0.07694227993488312\n",
      "fps: 5.898141390448629\n",
      "TIMESTEP 36011 / STATE explore / EPSILON 0.06412590999994519 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.2006104 / Loss  0.06939470022916794\n",
      "fps: 5.697164799669659\n",
      "TIMESTEP 36012 / STATE explore / EPSILON 0.06412491099994519 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.12886915 / Loss  0.011400250717997551\n",
      "fps: 5.570376351654656\n",
      "TIMESTEP 36013 / STATE explore / EPSILON 0.06412391199994519 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.16532831 / Loss  0.024101167917251587\n",
      "fps: 6.003612785022114\n",
      "TIMESTEP 36014 / STATE explore / EPSILON 0.06412291299994519 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.43919268 / Loss  0.04406119883060455\n",
      "fps: 6.040297413402037\n",
      "TIMESTEP 36015 / STATE explore / EPSILON 0.06412191399994518 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.25015008 / Loss  0.16501933336257935\n",
      "fps: 6.040219125686208\n",
      "TIMESTEP 36016 / STATE explore / EPSILON 0.06412091499994518 / ACTION 0 / REWARD 0.1 / Q_MAX  0.62460065 / Loss  0.03192722052335739\n",
      "----------Random Action----------\n",
      "fps: 6.266431007770469\n",
      "TIMESTEP 36017 / STATE explore / EPSILON 0.06411991599994518 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.3787894 / Loss  0.4521128535270691\n",
      "fps: 6.596632721267644\n",
      "TIMESTEP 36018 / STATE explore / EPSILON 0.06411891699994518 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.48954093 / Loss  0.044090040028095245\n",
      "\n",
      "CURRENT SCORE: 4.163336342344337e-17\n",
      "\n",
      "\n",
      "fps: 5.966157242509417\n",
      "TIMESTEP 36019 / STATE explore / EPSILON 0.06411791799994518 / ACTION 0 / REWARD -1 / Q_MAX  0.71289104 / Loss  0.03715617209672928\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.510967923340818\n",
      "TIMESTEP 36020 / STATE explore / EPSILON 0.06411691899994518 / ACTION 1 / REWARD -1 / Q_MAX  -0.21511637 / Loss  0.04869445785880089\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.633227454956921\n",
      "TIMESTEP 36021 / STATE explore / EPSILON 0.06411591999994518 / ACTION 1 / REWARD -1 / Q_MAX  -0.18878892 / Loss  0.011758247390389442\n",
      "fps: 6.344566365041764\n",
      "TIMESTEP 36022 / STATE explore / EPSILON 0.06411492099994517 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.0313813 / Loss  0.12447379529476166\n",
      "fps: 6.041428462370563\n",
      "TIMESTEP 36023 / STATE explore / EPSILON 0.06411392199994517 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.008436553 / Loss  0.013664386235177517\n",
      "fps: 6.684469377022009\n",
      "TIMESTEP 36024 / STATE explore / EPSILON 0.06411292299994517 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.13316888 / Loss  0.021744493395090103\n",
      "fps: 6.427490625340009\n",
      "TIMESTEP 36025 / STATE explore / EPSILON 0.06411192399994517 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.21329975 / Loss  0.013921542093157768\n",
      "fps: 6.385987777063876\n",
      "TIMESTEP 36026 / STATE explore / EPSILON 0.06411092499994517 / ACTION 1 / REWARD -0.05 / Q_MAX  0.52236676 / Loss  0.06811673194169998\n",
      "fps: 6.306465387622541\n",
      "TIMESTEP 36027 / STATE explore / EPSILON 0.06410992599994517 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.2140253 / Loss  0.0143907330930233\n",
      "fps: 5.539748180296884\n",
      "TIMESTEP 36028 / STATE explore / EPSILON 0.06410892699994516 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.29059458 / Loss  2.369401693344116\n",
      "fps: 6.151412933163598\n",
      "TIMESTEP 36029 / STATE explore / EPSILON 0.06410792799994516 / ACTION 0 / REWARD 0.1 / Q_MAX  0.24144733 / Loss  0.06007545441389084\n",
      "fps: 6.640139695626913\n",
      "TIMESTEP 36030 / STATE explore / EPSILON 0.06410692899994516 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.07856996 / Loss  0.0775473341345787\n",
      "fps: 6.1515302178985705\n",
      "TIMESTEP 36031 / STATE explore / EPSILON 0.06410592999994516 / ACTION 1 / REWARD -0.05 / Q_MAX  0.107051566 / Loss  0.04340880364179611\n",
      "fps: 6.1507995166531755\n",
      "TIMESTEP 36032 / STATE explore / EPSILON 0.06410493099994516 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.10328522 / Loss  0.1817108690738678\n",
      "fps: 6.46916725919368\n",
      "TIMESTEP 36033 / STATE explore / EPSILON 0.06410393199994516 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.2377094 / Loss  0.024305257946252823\n",
      "fps: 6.510993685073301\n",
      "TIMESTEP 36034 / STATE explore / EPSILON 0.06410293299994516 / ACTION 0 / REWARD 0.1 / Q_MAX  0.5826186 / Loss  0.061242133378982544\n",
      "fps: 6.511044221905887\n",
      "TIMESTEP 36035 / STATE explore / EPSILON 0.06410193399994515 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.08391937 / Loss  0.025172272697091103\n",
      "fps: 6.6843308902392575\n",
      "TIMESTEP 36036 / STATE explore / EPSILON 0.06410093499994515 / ACTION 0 / REWARD 0.1 / Q_MAX  0.4968999 / Loss  0.03712473064661026\n",
      "fps: 6.266945974294534\n",
      "TIMESTEP 36037 / STATE explore / EPSILON 0.06409993599994515 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.15807272 / Loss  0.0558595135807991\n",
      "fps: 6.684469377022009\n",
      "TIMESTEP 36038 / STATE explore / EPSILON 0.06409893699994515 / ACTION 0 / REWARD 0.1 / Q_MAX  0.8135572 / Loss  0.03925006091594696\n",
      "\n",
      "CURRENT SCORE: 0.3500000000000001\n",
      "\n",
      "\n",
      "fps: 5.863428640926673\n",
      "TIMESTEP 36039 / STATE explore / EPSILON 0.06409793799994515 / ACTION 1 / REWARD -1 / Q_MAX  0.06455703 / Loss  0.015220173634588718\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.968237164026404\n",
      "TIMESTEP 36040 / STATE explore / EPSILON 0.06409693899994515 / ACTION 0 / REWARD -1 / Q_MAX  0.12133336 / Loss  0.025343801826238632\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.968415510494486\n",
      "TIMESTEP 36041 / STATE explore / EPSILON 0.06409593999994515 / ACTION 0 / REWARD -1 / Q_MAX  14.080631 / Loss  0.4061989486217499\n",
      "----------Random Action----------\n",
      "fps: 6.040245221365999\n",
      "TIMESTEP 36042 / STATE explore / EPSILON 0.06409494099994514 / ACTION 1 / REWARD -0.05 / Q_MAX  0.45238236 / Loss  0.224045068025589\n",
      "----------Random Action----------\n",
      "fps: 6.5531699482221475\n",
      "TIMESTEP 36043 / STATE explore / EPSILON 0.06409394199994514 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.16844076 / Loss  2.930589199066162\n",
      "----------Random Action----------\n",
      "fps: 6.952200044421902\n",
      "TIMESTEP 36044 / STATE explore / EPSILON 0.06409294299994514 / ACTION 0 / REWARD 0.1 / Q_MAX  0.021456156 / Loss  0.03359558433294296\n",
      "fps: 6.553272336383181\n",
      "TIMESTEP 36045 / STATE explore / EPSILON 0.06409194399994514 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.5172472 / Loss  0.051335837692022324\n",
      "----------Random Action----------\n",
      "fps: 6.510751119196786\n",
      "TIMESTEP 36046 / STATE explore / EPSILON 0.06409094499994514 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.38860568 / Loss  0.04962197318673134\n",
      "fps: 5.6649239126471\n",
      "TIMESTEP 36047 / STATE explore / EPSILON 0.06408994599994514 / ACTION 1 / REWARD -0.05 / Q_MAX  1.004303 / Loss  0.5294976830482483\n",
      "fps: 6.189392910899271\n",
      "TIMESTEP 36048 / STATE explore / EPSILON 0.06408894699994513 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.21735749 / Loss  0.08040259033441544\n",
      "fps: 6.553405445775828\n",
      "TIMESTEP 36049 / STATE explore / EPSILON 0.06408794799994513 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.13271967 / Loss  0.11833503097295761\n",
      "fps: 6.6845119894942515\n",
      "TIMESTEP 36050 / STATE explore / EPSILON 0.06408694899994513 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.10333638 / Loss  0.03184778243303299\n",
      "fps: 6.384675453965625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 36051 / STATE explore / EPSILON 0.06408594999994513 / ACTION 1 / REWARD -0.05 / Q_MAX  1.0337825 / Loss  0.03445824235677719\n",
      "fps: 6.189456845907862\n",
      "TIMESTEP 36052 / STATE explore / EPSILON 0.06408495099994513 / ACTION 1 / REWARD -0.05 / Q_MAX  0.45047513 / Loss  0.06647434085607529\n",
      "fps: 6.003982324320378\n",
      "TIMESTEP 36053 / STATE explore / EPSILON 0.06408395199994513 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.2738419 / Loss  0.05690930038690567\n",
      "fps: 6.510831972480767\n",
      "TIMESTEP 36054 / STATE explore / EPSILON 0.06408295299994513 / ACTION 1 / REWARD -0.05 / Q_MAX  16.426394 / Loss  0.029150966554880142\n",
      "fps: 6.189420311455501\n",
      "TIMESTEP 36055 / STATE explore / EPSILON 0.06408195399994512 / ACTION 1 / REWARD -0.05 / Q_MAX  16.225077 / Loss  0.25843170285224915\n",
      "fps: 6.3863669930674725\n",
      "TIMESTEP 36056 / STATE explore / EPSILON 0.06408095499994512 / ACTION 1 / REWARD -0.05 / Q_MAX  15.025753 / Loss  0.3304266929626465\n",
      "fps: 6.038888544940674\n",
      "TIMESTEP 36057 / STATE explore / EPSILON 0.06407995599994512 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.36556217 / Loss  0.16169223189353943\n",
      "fps: 5.9690100757101385\n",
      "TIMESTEP 36058 / STATE explore / EPSILON 0.06407895699994512 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.40080166 / Loss  0.1870240569114685\n",
      "----------Random Action----------\n",
      "fps: 6.5114990887045945\n",
      "TIMESTEP 36059 / STATE explore / EPSILON 0.06407795799994512 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.33454728 / Loss  3.7092206478118896\n",
      "fps: 6.113860577932225\n",
      "TIMESTEP 36060 / STATE explore / EPSILON 0.06407695899994512 / ACTION 0 / REWARD 0.1 / Q_MAX  -1.1802584 / Loss  0.02539282850921154\n",
      "fps: 6.7288875198932185\n",
      "TIMESTEP 36061 / STATE explore / EPSILON 0.06407595999994511 / ACTION 0 / REWARD 0.1 / Q_MAX  0.39065367 / Loss  1.3683576583862305\n",
      "fps: 4.5996479780671695\n",
      "TIMESTEP 36062 / STATE explore / EPSILON 0.06407496099994511 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.14512439 / Loss  0.9352287650108337\n",
      "\n",
      "CURRENT SCORE: -0.30000000000000004\n",
      "\n",
      "\n",
      "fps: 5.509172751405105\n",
      "TIMESTEP 36063 / STATE explore / EPSILON 0.06407396199994511 / ACTION 1 / REWARD -1 / Q_MAX  0.019592248 / Loss  0.030338870361447334\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.004085459685789\n",
      "TIMESTEP 36064 / STATE explore / EPSILON 0.06407296299994511 / ACTION 1 / REWARD -1 / Q_MAX  -0.1971466 / Loss  0.3239763081073761\n",
      "fps: 5.897710127605723\n",
      "TIMESTEP 36065 / STATE explore / EPSILON 0.06407196399994511 / ACTION 1 / REWARD -0.05 / Q_MAX  0.16600332 / Loss  0.38722729682922363\n",
      "fps: 6.346553605122875\n",
      "TIMESTEP 36066 / STATE explore / EPSILON 0.06407096499994511 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.3681766 / Loss  0.14686903357505798\n",
      "fps: 6.553405445775828\n",
      "TIMESTEP 36067 / STATE explore / EPSILON 0.0640699659999451 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.052711356 / Loss  0.0342293456196785\n",
      "----------Random Action----------\n",
      "fps: 5.829502874234186\n",
      "TIMESTEP 36068 / STATE explore / EPSILON 0.0640689669999451 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.13223508 / Loss  0.05369849503040314\n",
      "fps: 6.640171232442976\n",
      "TIMESTEP 36069 / STATE explore / EPSILON 0.0640679679999451 / ACTION 1 / REWARD -0.05 / Q_MAX  0.27038592 / Loss  0.1332799196243286\n",
      "fps: 6.227854040610268\n",
      "TIMESTEP 36070 / STATE explore / EPSILON 0.0640669689999451 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.04380862 / Loss  0.056491605937480927\n",
      "fps: 6.386376717146955\n",
      "TIMESTEP 36071 / STATE explore / EPSILON 0.0640659699999451 / ACTION 0 / REWARD 0.1 / Q_MAX  0.056597654 / Loss  0.027927998453378677\n",
      "fps: 6.774873727380362\n",
      "TIMESTEP 36072 / STATE explore / EPSILON 0.0640649709999451 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.19502574 / Loss  0.08826066553592682\n",
      "fps: 6.468847983836763\n",
      "TIMESTEP 36073 / STATE explore / EPSILON 0.0640639719999451 / ACTION 0 / REWARD 0.1 / Q_MAX  0.25578475 / Loss  0.27782559394836426\n",
      "fps: 5.968092796120594\n",
      "TIMESTEP 36074 / STATE explore / EPSILON 0.0640629729999451 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.056304537 / Loss  0.036927975714206696\n",
      "fps: 6.553282575375255\n",
      "TIMESTEP 36075 / STATE explore / EPSILON 0.0640619739999451 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.017093893 / Loss  0.031109735369682312\n",
      "fps: 6.267095798630723\n",
      "TIMESTEP 36076 / STATE explore / EPSILON 0.06406097499994509 / ACTION 0 / REWARD 0.1 / Q_MAX  0.20878918 / Loss  0.017036495730280876\n",
      "fps: 6.596487475603101\n",
      "TIMESTEP 36077 / STATE explore / EPSILON 0.06405997599994509 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.0006116517 / Loss  0.3766009211540222\n",
      "fps: 6.113709079089103\n",
      "TIMESTEP 36078 / STATE explore / EPSILON 0.06405897699994509 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.26930586 / Loss  0.0746099054813385\n",
      "fps: 6.640286869998179\n",
      "TIMESTEP 36079 / STATE explore / EPSILON 0.06405797799994509 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.56238025 / Loss  0.020289968699216843\n",
      "fps: 6.114047733928465\n",
      "TIMESTEP 36080 / STATE explore / EPSILON 0.06405697899994509 / ACTION 0 / REWARD 0.1 / Q_MAX  0.18562488 / Loss  0.08892226219177246\n",
      "fps: 6.684469377022009\n",
      "TIMESTEP 36081 / STATE explore / EPSILON 0.06405597999994508 / ACTION 0 / REWARD 0.1 / Q_MAX  0.5550284 / Loss  0.018449001014232635\n",
      "fps: 6.113905137844429\n",
      "TIMESTEP 36082 / STATE explore / EPSILON 0.06405498099994508 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.14233495 / Loss  0.022112054750323296\n",
      "fps: 6.346025473042694\n",
      "TIMESTEP 36083 / STATE explore / EPSILON 0.06405398199994508 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.111858204 / Loss  0.05107500031590462\n",
      "\n",
      "CURRENT SCORE: 0.7\n",
      "\n",
      "\n",
      "fps: 5.601487211265198\n",
      "TIMESTEP 36084 / STATE explore / EPSILON 0.06405298299994508 / ACTION 1 / REWARD -1 / Q_MAX  0.19204043 / Loss  0.04483545944094658\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.729430390553818\n",
      "TIMESTEP 36085 / STATE explore / EPSILON 0.06405198399994508 / ACTION 0 / REWARD -1 / Q_MAX  -0.20420235 / Loss  0.024911105632781982\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.72972780984256\n",
      "TIMESTEP 36086 / STATE explore / EPSILON 0.06405098499994508 / ACTION 1 / REWARD -1 / Q_MAX  0.14375024 / Loss  0.014453263022005558\n",
      "----------Random Action----------\n",
      "fps: 6.306019752618673\n",
      "TIMESTEP 36087 / STATE explore / EPSILON 0.06404998599994507 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.27695432 / Loss  0.08295167982578278\n",
      "fps: 6.729405724553972\n",
      "TIMESTEP 36088 / STATE explore / EPSILON 0.06404898699994507 / ACTION 0 / REWARD 0.1 / Q_MAX  0.1602187 / Loss  0.030322905629873276\n",
      "fps: 6.227854040610268\n",
      "TIMESTEP 36089 / STATE explore / EPSILON 0.06404798799994507 / ACTION 0 / REWARD 0.1 / Q_MAX  0.11998162 / Loss  0.022730033844709396\n",
      "fps: 6.639509022207659\n",
      "TIMESTEP 36090 / STATE explore / EPSILON 0.06404698899994507 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.15131526 / Loss  0.14017343521118164\n",
      "fps: 5.8985810117147395\n",
      "TIMESTEP 36091 / STATE explore / EPSILON 0.06404598999994507 / ACTION 0 / REWARD 0.1 / Q_MAX  0.22731705 / Loss  0.025426071137189865\n",
      "fps: 6.8211489078658705\n",
      "TIMESTEP 36092 / STATE explore / EPSILON 0.06404499099994507 / ACTION 0 / REWARD 0.1 / Q_MAX  0.62566775 / Loss  0.028292108327150345\n",
      "\n",
      "CURRENT SCORE: 0.6\n",
      "\n",
      "\n",
      "fps: 5.5092957702078245\n",
      "TIMESTEP 36093 / STATE explore / EPSILON 0.06404399199994507 / ACTION 0 / REWARD -1 / Q_MAX  0.3056974 / Loss  0.4604896306991577\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.004025296958696\n",
      "TIMESTEP 36094 / STATE explore / EPSILON 0.06404299299994506 / ACTION 0 / REWARD -1 / Q_MAX  0.11424355 / Loss  0.02940329723060131\n",
      "----------Random Action----------\n",
      "fps: 4.397453142839769\n",
      "TIMESTEP 36095 / STATE explore / EPSILON 0.06404199399994506 / ACTION 1 / REWARD -0.05 / Q_MAX  0.02779239 / Loss  0.010334480553865433\n",
      "fps: 5.633204757658132\n",
      "TIMESTEP 36096 / STATE explore / EPSILON 0.06404099499994506 / ACTION 1 / REWARD -0.05 / Q_MAX  0.12912962 / Loss  0.1881616711616516\n",
      "fps: 7.012092223895893\n",
      "TIMESTEP 36097 / STATE explore / EPSILON 0.06403999599994506 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.44429615 / Loss  0.02798907645046711\n",
      "fps: 6.8675882823923144\n",
      "TIMESTEP 36098 / STATE explore / EPSILON 0.06403899699994506 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.18050364 / Loss  0.0744209885597229\n",
      "fps: 7.65273619314004\n",
      "TIMESTEP 36099 / STATE explore / EPSILON 0.06403799799994506 / ACTION 0 / REWARD 0.1 / Q_MAX  0.28715697 / Loss  0.07163490355014801\n",
      "fps: 6.468907845565633\n",
      "TIMESTEP 36100 / STATE explore / EPSILON 0.06403699899994506 / ACTION 1 / REWARD -0.05 / Q_MAX  0.1580507 / Loss  0.04132317006587982\n",
      "fps: 6.962378656892819\n",
      "TIMESTEP 36101 / STATE explore / EPSILON 0.06403599999994505 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.3958252 / Loss  0.041767798364162445\n",
      "fps: 6.597130754965971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 36102 / STATE explore / EPSILON 0.06403500099994505 / ACTION 0 / REWARD 0.1 / Q_MAX  0.37395182 / Loss  0.057187944650650024\n",
      "fps: 7.111183449809942\n",
      "TIMESTEP 36103 / STATE explore / EPSILON 0.06403400199994505 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.39240265 / Loss  0.03720296919345856\n",
      "fps: 7.161829566274565\n",
      "TIMESTEP 36104 / STATE explore / EPSILON 0.06403300299994505 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.12717608 / Loss  0.15832139551639557\n",
      "fps: 6.22788178276157\n",
      "TIMESTEP 36105 / STATE explore / EPSILON 0.06403200399994505 / ACTION 1 / REWARD -0.05 / Q_MAX  0.033335596 / Loss  0.014428951777517796\n",
      "fps: 6.427204996736062\n",
      "TIMESTEP 36106 / STATE explore / EPSILON 0.06403100499994505 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.17449912 / Loss  0.008869432844221592\n",
      "----------Random Action----------\n",
      "fps: 6.9637079970579885\n",
      "TIMESTEP 36107 / STATE explore / EPSILON 0.06403000599994504 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.19029002 / Loss  0.03915241360664368\n",
      "fps: 6.64027635732673\n",
      "TIMESTEP 36108 / STATE explore / EPSILON 0.06402900699994504 / ACTION 0 / REWARD 0.1 / Q_MAX  0.066018686 / Loss  0.0291209127753973\n",
      "fps: 6.914937920096545\n",
      "TIMESTEP 36109 / STATE explore / EPSILON 0.06402800799994504 / ACTION 0 / REWARD 0.1 / Q_MAX  0.086516134 / Loss  0.02995159849524498\n",
      "fps: 6.914527533288383\n",
      "TIMESTEP 36110 / STATE explore / EPSILON 0.06402700899994504 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.15596464 / Loss  0.035523671656847\n",
      "fps: 6.639708722494855\n",
      "TIMESTEP 36111 / STATE explore / EPSILON 0.06402600999994504 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.5242439 / Loss  0.03953573852777481\n",
      "----------Random Action----------\n",
      "fps: 7.483854851573833\n",
      "TIMESTEP 36112 / STATE explore / EPSILON 0.06402501099994504 / ACTION 0 / REWARD 0.1 / Q_MAX  0.115697585 / Loss  0.01521269604563713\n",
      "\n",
      "CURRENT SCORE: 0.8999999999999999\n",
      "\n",
      "\n",
      "fps: 6.427273939247202\n",
      "TIMESTEP 36113 / STATE explore / EPSILON 0.06402401199994504 / ACTION 0 / REWARD -1 / Q_MAX  0.1645818 / Loss  0.06895528733730316\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.640707404152912\n",
      "TIMESTEP 36114 / STATE explore / EPSILON 0.06402301299994503 / ACTION 0 / REWARD -1 / Q_MAX  12.31599 / Loss  0.011638697236776352\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.509201696511486\n",
      "TIMESTEP 36115 / STATE explore / EPSILON 0.06402201399994503 / ACTION 1 / REWARD -1 / Q_MAX  0.06912663 / Loss  0.06071820482611656\n",
      "fps: 7.265770377415035\n",
      "TIMESTEP 36116 / STATE explore / EPSILON 0.06402101499994503 / ACTION 1 / REWARD -0.05 / Q_MAX  0.06831768 / Loss  0.04100650176405907\n",
      "fps: 6.820860497981048\n",
      "TIMESTEP 36117 / STATE explore / EPSILON 0.06402001599994503 / ACTION 1 / REWARD -0.05 / Q_MAX  0.14602987 / Loss  0.07849554717540741\n",
      "fps: 7.427267109012124\n",
      "TIMESTEP 36118 / STATE explore / EPSILON 0.06401901699994503 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.0887578 / Loss  0.03687494993209839\n",
      "fps: 7.4272539568421765\n",
      "TIMESTEP 36119 / STATE explore / EPSILON 0.06401801799994503 / ACTION 0 / REWARD 0.1 / Q_MAX  0.20424578 / Loss  0.024708516895771027\n",
      "fps: 7.595982425829539\n",
      "TIMESTEP 36120 / STATE explore / EPSILON 0.06401701899994502 / ACTION 0 / REWARD 0.1 / Q_MAX  0.038330648 / Loss  0.10387784987688065\n",
      "fps: 7.213152644974891\n",
      "TIMESTEP 36121 / STATE explore / EPSILON 0.06401601999994502 / ACTION 0 / REWARD 0.1 / Q_MAX  0.2322157 / Loss  0.020495038479566574\n",
      "fps: 7.213797753118185\n",
      "TIMESTEP 36122 / STATE explore / EPSILON 0.06401502099994502 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.15851484 / Loss  0.015604330226778984\n",
      "fps: 7.372565287472294\n",
      "TIMESTEP 36123 / STATE explore / EPSILON 0.06401402199994502 / ACTION 0 / REWARD 0.1 / Q_MAX  0.5623633 / Loss  0.08932027965784073\n",
      "\n",
      "CURRENT SCORE: 0.5\n",
      "\n",
      "\n",
      "fps: 6.510933041909735\n",
      "TIMESTEP 36124 / STATE explore / EPSILON 0.06401302299994502 / ACTION 0 / REWARD -1 / Q_MAX  0.2690455 / Loss  0.016473161056637764\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.4271951479266365\n",
      "TIMESTEP 36125 / STATE explore / EPSILON 0.06401202399994502 / ACTION 0 / REWARD -1 / Q_MAX  -0.25749195 / Loss  0.09790287911891937\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.151548262100908\n",
      "TIMESTEP 36126 / STATE explore / EPSILON 0.06401102499994502 / ACTION 1 / REWARD -1 / Q_MAX  -0.42518637 / Loss  0.041639991104602814\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 5.419809687239948\n",
      "TIMESTEP 36127 / STATE explore / EPSILON 0.06401002599994501 / ACTION 0 / REWARD -1 / Q_MAX  12.688929 / Loss  4.608448028564453\n",
      "fps: 6.729459708796278\n",
      "TIMESTEP 36128 / STATE explore / EPSILON 0.06400902699994501 / ACTION 1 / REWARD -0.05 / Q_MAX  -0.1479477 / Loss  0.03019076958298683\n",
      "fps: 6.820538838803679\n",
      "TIMESTEP 36129 / STATE explore / EPSILON 0.06400802799994501 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.04040141 / Loss  0.03588111698627472\n",
      "fps: 7.3192252723570075\n",
      "TIMESTEP 36130 / STATE explore / EPSILON 0.06400702899994501 / ACTION 0 / REWARD 0.1 / Q_MAX  0.089965716 / Loss  0.059867002069950104\n",
      "fps: 7.596037452233913\n",
      "TIMESTEP 36131 / STATE explore / EPSILON 0.06400602999994501 / ACTION 0 / REWARD 0.1 / Q_MAX  0.037194528 / Loss  0.047456976026296616\n",
      "fps: 7.213351127498865\n",
      "TIMESTEP 36132 / STATE explore / EPSILON 0.064005030999945 / ACTION 0 / REWARD 0.1 / Q_MAX  0.39522997 / Loss  0.03506549447774887\n",
      "fps: 7.213487590570519\n",
      "TIMESTEP 36133 / STATE explore / EPSILON 0.064004031999945 / ACTION 0 / REWARD 0.1 / Q_MAX  0.5120263 / Loss  4.551201820373535\n",
      "fps: 7.318854893366045\n",
      "TIMESTEP 36134 / STATE explore / EPSILON 0.064003032999945 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.18414256 / Loss  0.04233025014400482\n",
      "\n",
      "CURRENT SCORE: 0.5499999999999999\n",
      "\n",
      "\n",
      "fps: 6.427224694445466\n",
      "TIMESTEP 36135 / STATE explore / EPSILON 0.064002033999945 / ACTION 0 / REWARD -1 / Q_MAX  0.3405843 / Loss  0.03507115691900253\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.640286869998179\n",
      "TIMESTEP 36136 / STATE explore / EPSILON 0.064001034999945 / ACTION 0 / REWARD -1 / Q_MAX  0.42520997 / Loss  0.04929041117429733\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.227900277666414\n",
      "TIMESTEP 36137 / STATE explore / EPSILON 0.064000035999945 / ACTION 1 / REWARD -1 / Q_MAX  0.07733902 / Loss  0.14576677978038788\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.345939059797895\n",
      "TIMESTEP 36138 / STATE explore / EPSILON 0.063999036999945 / ACTION 0 / REWARD -1 / Q_MAX  0.5538496 / Loss  0.04117412120103836\n",
      "fps: 6.684533295934109\n",
      "TIMESTEP 36139 / STATE explore / EPSILON 0.063998037999945 / ACTION 1 / REWARD -0.05 / Q_MAX  0.31637615 / Loss  0.11673086881637573\n",
      "fps: 6.915040524410269\n",
      "TIMESTEP 36140 / STATE explore / EPSILON 0.063997038999945 / ACTION 1 / REWARD -0.05 / Q_MAX  0.6537967 / Loss  0.027945224195718765\n",
      "fps: 7.712924924880747\n",
      "TIMESTEP 36141 / STATE explore / EPSILON 0.06399603999994499 / ACTION 0 / REWARD 0.1 / Q_MAX  0.06017063 / Loss  3.683668851852417\n",
      "fps: 7.2634300508264715\n",
      "TIMESTEP 36142 / STATE explore / EPSILON 0.06399504099994499 / ACTION 0 / REWARD 0.1 / Q_MAX  -1.2408552 / Loss  0.04336187243461609\n",
      "fps: 6.5120652339298415\n",
      "TIMESTEP 36143 / STATE explore / EPSILON 0.06399404199994499 / ACTION 0 / REWARD 0.1 / Q_MAX  0.36508116 / Loss  0.13748715817928314\n",
      "fps: 6.684213713597262\n",
      "TIMESTEP 36144 / STATE explore / EPSILON 0.06399304299994499 / ACTION 0 / REWARD 0.1 / Q_MAX  0.583047 / Loss  0.12716124951839447\n",
      "fps: 7.060190749362878\n",
      "TIMESTEP 36145 / STATE explore / EPSILON 0.06399204399994499 / ACTION 0 / REWARD 0.1 / Q_MAX  0.28611216 / Loss  0.1855652779340744\n",
      "fps: 7.213611652388971\n",
      "TIMESTEP 36146 / STATE explore / EPSILON 0.06399104499994498 / ACTION 0 / REWARD 0.1 / Q_MAX  1.0842425 / Loss  0.8542827367782593\n",
      "fps: 7.0113889200000665\n",
      "TIMESTEP 36147 / STATE explore / EPSILON 0.06399004599994498 / ACTION 0 / REWARD 0.1 / Q_MAX  -0.30486867 / Loss  0.1662999987602234\n",
      "\n",
      "CURRENT SCORE: 0.6\n",
      "\n",
      "\n",
      "fps: 6.867565793028469\n",
      "TIMESTEP 36148 / STATE explore / EPSILON 0.06398904699994498 / ACTION 0 / REWARD -1 / Q_MAX  -0.2095773 / Loss  0.021076057106256485\n",
      "----------Random Action----------\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.004111244080782\n",
      "TIMESTEP 36149 / STATE explore / EPSILON 0.06398804799994498 / ACTION 1 / REWARD -1 / Q_MAX  -0.09022945 / Loss  0.029059642925858498\n",
      "\n",
      "CURRENT SCORE: 0\n",
      "\n",
      "\n",
      "fps: 6.189548183925754\n",
      "TIMESTEP 36150 / STATE explore / EPSILON 0.06398704899994498 / ACTION 0 / REWARD -1 / Q_MAX  1.9650686 / Loss  1.3897966146469116\n",
      "fps: 6.189310710686191\n"
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
